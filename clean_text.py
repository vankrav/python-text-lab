
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer
import re
def clean_stopwords_punctuation(text):

    # Создание токенизатора для удаления пунктуации
    tokenizer = RegexpTokenizer(r'\w+')
    words = tokenizer.tokenize(text)
    stopwords2 = {
        "а", "без", "более", "бы", "был", "была", "были", "было", "быть", "в", "вам", "вас", "весь", "во", "вот", "все",
        "всего", "всех", "вы", "где", "да", "даже", "для", "до", "его", "ее", "её", "если", "есть", "еще", "же", "за",
        "здесь", "и", "из", "или", "им", "их", "к", "как", "какая", "какой", "когда", "конечно", "кто", "ли", "либо",
        "мы", "на", "надо", "наконец","котор" "наш", "не", "него", "нее", "неё", "нет", "ни", "нибудь", "никогда", "ним",
        "них", "ничего", "но", "ну", "о", "об", "однако", "он", "она", "они", "оно", "от", "очень", "по", "под",
        "после",
        "потом", "потому", "почти", "при", "про", "раз", "разве", "с", "сам", "свою", "себе", "себя", "сейчас",
        "сказал",
        "со", "совсем", "так", "также","такой", "там", "тебя", "тем", "теперь", "то", "тогда", "того", "тоже", "той", "только",
        "том", "тот", "ты", "у", "уж", "уже", "хорошо", "хоть", "чего", "чем", "через", "что", "чтоб", "чтобы", "чуть",
        "эти", "это", "этот", "эту", "я"
    }
    # Получение русских стоп-слов
    stop_words = set(stopwords.words('russian'))

    # Удаление стоп-слов и приведение к нижнему регистру
    filtered_words = [word for word in words if word.lower() not in stop_words and not re.match(r'^\d+$', word) and word.lower() not in stopwords2]

    return filtered_words